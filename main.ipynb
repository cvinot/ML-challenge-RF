{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Dataset concaténés de chaque WF\n",
    "\n",
    "X_train_tot = pd.read_csv('X_train_v2.csv', sep=',', index_col='ID', header=0)\n",
    "Y_train_tot = pd.read_csv('Y_train_sl9m6Jh.csv', sep=',', index_col='ID', header=0)\n",
    "\n",
    "for i in range(6):\n",
    "    X_train_tot[X_train_tot['WF']==('WF'+str(i))]=X_train_tot.loc[X_train_tot['WF']==('WF'+str(i))].interpolate()\n",
    "\n",
    "X_test_tot = pd.read_csv('X_test_v2.csv', sep=',', index_col='ID', header=0)\n",
    "Y_test_tot = pd.read_csv('Y_test_random.csv', sep=',', index_col='ID', header=0)\n",
    "\n",
    "\n",
    "# On récupère les data spécifiques à chaque WF\n",
    "# WF_X_train[i] contient alors le DataFrame de l'éolienne i\n",
    "\n",
    "WF_X_train = [None]\n",
    "WF_Y_train = [None]\n",
    "\n",
    "WF_X_test = [None]\n",
    "WF_Y_test = [None]\n",
    "\n",
    "WF_X_train.append(X_train_tot.loc[X_train_tot['WF']==('WF1')])\n",
    "WF_Y_train.append(Y_train_tot.iloc[:WF_X_train[1].shape[0]])\n",
    "\n",
    "WF_X_test.append(X_test_tot.loc[X_test_tot['WF']==('WF1')])\n",
    "WF_Y_test.append(Y_test_tot.iloc[:WF_X_test[1].shape[0]])\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in range(1,6):\n",
    "    WF_X_train.append(X_train_tot.loc[X_train_tot['WF']==('WF'+str(i+1))])\n",
    "    k += WF_X_train[i].shape[0]\n",
    "    WF_Y_train.append(Y_train_tot.iloc[k:k+WF_X_train[i+1].shape[0]])\n",
    "\n",
    "k = 0\n",
    "\n",
    "for i in range(1,6):\n",
    "    WF_X_test.append(X_test_tot.loc[X_test_tot['WF']==('WF'+str(i+1))])\n",
    "    k += WF_X_test[i].shape[0]\n",
    "    WF_Y_test.append(Y_test_tot.iloc[k:k+WF_X_test[i+1].shape[0]])\n",
    "    \n",
    "\n",
    "# Maintenant il nous faut récupérer les données des différentes stations, pour cela on procède de la même manière\n",
    "# On doit identifier le numéro dans le nom de la colonne NWPi..\n",
    "\n",
    "# On parcourt les différentes éoliennes\n",
    "\n",
    "for i in range(1,len(WF_X_train)) :\n",
    "    extraction_stations = [None]\n",
    "    WFi = WF_X_train[i]\n",
    "    # On traite chaque numéro de station\n",
    "    k=1\n",
    "    # On parcourt les données de gauche à droite en extrayant les données relatives à la station k entre n1 et n2\n",
    "    n1 = 2 # On ne prend pas les colonnes WF et Time, on les rajoutera après\n",
    "    n2 = 3 \n",
    "    while k < 5 :\n",
    "        # Si k vaut 4, alors on prend tous les derniers éléments\n",
    "        if k == 4 :\n",
    "            n2 = WFi.shape[1]-1\n",
    "            donnees_station = WFi.iloc[:,n1:n2+1]\n",
    "            \n",
    "        # Si k ne vaut pas 4 alors on incrémente n2 librement jusqu'à passer à une nouvelle station\n",
    "        else :\n",
    "            nom_col = WFi.columns[n2]\n",
    "            while int(nom_col.split('NWP')[1].split('_')[0]) == k :\n",
    "                n2 +=1\n",
    "                nom_col = WFi.columns[n2]\n",
    "            \n",
    "            #On extrait les données\n",
    "            donnees_station = WFi.iloc[:,n1:n2]\n",
    "\n",
    "        #On insert le temps\n",
    "        donnees_station.insert(0, 'Time', WFi.iloc[:,1])\n",
    "        extraction_stations.append(donnees_station)\n",
    "        \n",
    "        n1 = n2\n",
    "        n2 += 1\n",
    "        k += 1\n",
    "        \n",
    "    WF_X_train[i] = extraction_stations\n",
    "            \n",
    "\n",
    "for i in range(1,len(WF_X_test)) :\n",
    "    extraction_stations = [None]\n",
    "    WFi = WF_X_test[i]\n",
    "    # On traite chaque numéro de station\n",
    "    k=1\n",
    "    # On parcourt les données de gauche à droite en extrayant les données relatives à la station k entre n1 et n2\n",
    "    n1 = 2 # On ne prend pas les colonnes WF et Time, on les rajoutera après\n",
    "    n2 = 3 \n",
    "    while k < 5 :\n",
    "        # Si k vaut 4, alors on prend tous les derniers éléments\n",
    "        if k == 4 :\n",
    "            n2 = WFi.shape[1]-1\n",
    "            donnees_station = WFi.iloc[:,n1:n2+1]\n",
    "            \n",
    "        # Si k ne vaut pas 4 alors on incrémente n2 librement jusqu'à passer à une nouvelle station\n",
    "        else :\n",
    "            nom_col = WFi.columns[n2]\n",
    "            while int(nom_col.split('NWP')[1].split('_')[0]) == k :\n",
    "                n2 +=1\n",
    "                nom_col = WFi.columns[n2]\n",
    "            \n",
    "            #On extrait les données\n",
    "            donnees_station = WFi.iloc[:,n1:n2]\n",
    "\n",
    "        #On insert le temps\n",
    "        donnees_station.insert(0, 'Time', WFi.iloc[:,1])\n",
    "        extraction_stations.append(donnees_station)\n",
    "        \n",
    "        n1 = n2\n",
    "        n2 += 1\n",
    "        k += 1\n",
    "        \n",
    "    WF_X_test[i] = extraction_stations\n",
    "            \n",
    "\n",
    "\n",
    "# WF_X_train[i][j] contient les données de l'éolienne i pour la station j\n",
    "# WF_Y_train[i] contient les données resultats correspondants à l'éolienne i\n",
    "\n",
    "# Idem pour les test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separation(df):\n",
    "    #df_U=pd.DataFrame(columns=[])\n",
    "    #Création de 4 dataframe vierge qui prennent le premier argument de Df qui mets la 1 ere colonne de df\n",
    "    df_U=df.iloc[:,[0]]\n",
    "    df_V=df.iloc[:,[0]]\n",
    "    df_T=df.iloc[:,[0]]\n",
    "    df_CLCT=df.iloc[:,[0]]\n",
    "    #nom colonne=liste de toute les collones\n",
    "    Nom_Colonne=list(df)\n",
    "    for Colonne in Nom_Colonne:\n",
    "        if '_U'in Colonne:\n",
    "            df_U = pd.merge(df_U, df[[Colonne]], on='ID')  \n",
    "        if '_V' in Colonne:\n",
    "            df_V = pd.merge(df_V, df[[Colonne]], on='ID')\n",
    "        if '_T' in Colonne:\n",
    "            df_T = pd.merge(df_T, df[[Colonne]], on='ID')\n",
    "        if '_CLCT' in Colonne:\n",
    "            df_CLCT = pd.merge(df_CLCT, df[[Colonne]], on='ID')\n",
    "    return(df_U,df_V,df_T,df_CLCT)\n",
    "\n",
    "def valeurs_finales(df):\n",
    "    \n",
    "    [df_U,df_V,df_T,df_CLCT]=separation(df)\n",
    "    \n",
    "    df_U=df_U.fillna(method='ffill',axis=1)\n",
    "    df_V=df_V.fillna(method='ffill',axis=1)\n",
    "    df_T=df_T.fillna(method='ffill',axis=1)\n",
    "    df_CLCT=df_CLCT.fillna(method='ffill',axis=1)\n",
    "    df_final=df.iloc[:,[0]]\n",
    "    df_final = pd.merge(df_final, df_U.iloc[:,[-1]], on='ID') \n",
    "    df_final = pd.merge(df_final, df_V.iloc[:,[-1]], on='ID')\n",
    "\n",
    "    if len(list(df_T))>1:\n",
    "        df_final = pd.merge(df_final, df_T.iloc[:,[-1]], on='ID')\n",
    "    if len(list(df_CLCT))>1:\n",
    "        df_final = pd.merge(df_final, df_CLCT.iloc[:,[-1]], on='ID')\n",
    "    df_final=df_final.dropna()\n",
    "    Liste_compteur=[]\n",
    "    for x in df_final.itertuples():\n",
    "        compteur=None\n",
    "        for k in range (len(list(df_final))):\n",
    "            if k>0 and x[k]!=x[1]:\n",
    "                compteur=1\n",
    "        Liste_compteur.append(compteur)\n",
    "    df_final['Compteur']=Liste_compteur\n",
    "\n",
    "    df_final=df_final.dropna()\n",
    "    del df_final['Compteur']\n",
    "    return(df_final)\n",
    "\n",
    "\n",
    "#separation(df) sert à séparer un dataframe en 2,3 ou 4 dataframe avec les données de U, V, T et CLCT quand elles y sont\n",
    "#dernieres_valeurs(df) donne les dernieres valeurs mesurées de u,v,t,et clct quand elles existent du dataframe d'entrée, fonctionne qq soit WF ou NWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait une concaténation de la WF[i]\n",
    "\n",
    "def concatene(i,df):\n",
    "    df_1=valeurs_finales(df[i][1])\n",
    "    df_2=valeurs_finales(df[i][2])\n",
    "    df_3=valeurs_finales(df[i][3])\n",
    "    df_4=valeurs_finales(df[i][4])\n",
    "    df_final = df[i][1].iloc[:,[0]]\n",
    "    df_final = pd.merge(df_final, df_1.iloc[:,1:], on='ID', how='outer')\n",
    "    df_final = pd.merge(df_final, df_2.iloc[:,1:], on='ID', how='outer')\n",
    "    df_final = pd.merge(df_final, df_3.iloc[:,1:], on='ID', how='outer')\n",
    "    df_final = pd.merge(df_final, df_4.iloc[:,1:], on='ID', how='outer')\n",
    "    df_final = df_final.fillna(method=\"bfill\")\n",
    "    df_final = df_final.fillna(method=\"ffill\")\n",
    "    del df_final['Time']\n",
    "    df_final.columns=['NWP1_U','NWP1_V','NWP1_T','NWP2_U','NWP2_V','NWP3_U','NWP3_V','NWP3_T','NWP4_U','NWP4_V','NWP4_CLCT']\n",
    "    return(df_final)\n",
    "    \n",
    "#concatene(i) prend en entrée le numéro de la WF et sors un dataframe avec les principales valeurs pour chaque station météo. \n",
    "#les données manquantes sont complété par les valeurs précédentes, ca peut etre améliorer (en faisant qqc de linéaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fonction de normalisation des données : prend un dataframe et le normalise par rapport à chacune de ses colonnes\n",
    "#attention: il ne doit pas y avoir de blancs\n",
    "\n",
    "def normalisation(df_init):\n",
    "    df=df_init.copy()\n",
    "    if 'Time' in list(df):\n",
    "        del(df['Time'])\n",
    "    if 'WF' in list(df):\n",
    "        del(df['WF'])\n",
    "    X_scaled = pd.DataFrame(preprocessing.scale(df))\n",
    "    return(X_scaled)\n",
    "    \n",
    "\n",
    "#pour linstant n'arrive pas à merge car normalise aussi ID\n",
    "#pas besoin, ca le fait dans les arguments de ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#faire une régression ridge\n",
    "def regression_ridge(X,y,alpha):\n",
    "    clf = Ridge(alpha=1.0, normalize=True)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "#fonctionne mais sert à rien: lappliquer directement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retourne la norme et la direction du vent pour une valeur de u et une valeur de v en entrée\n",
    "def calcule_force_direction(u,v):\n",
    "    \n",
    "    force = np.sqrt(u**2 + v**2)\n",
    "    \n",
    "    if u > 0 :\n",
    "        if v < 0 :\n",
    "            direction = np.arctan(v/u) + 2*np.pi\n",
    "            \n",
    "        elif v>=0 :\n",
    "            direction = np.arctan(v/u)\n",
    "    \n",
    "    elif u < 0 :\n",
    "        if v < 0 :\n",
    "            direction = np.arctan(v/u) + np.pi\n",
    "            \n",
    "        elif v>=0 :\n",
    "            direction = np.arctan(v/u) + np.pi\n",
    "    \n",
    "    elif u == 0 :\n",
    "        if v > 0 :\n",
    "            direction = np.pi/2\n",
    "        \n",
    "        if v< 0 :\n",
    "            direction = 3/2 * np.pi/2\n",
    "        \n",
    "        if v == 0 :\n",
    "            direction = 0\n",
    "    \n",
    "    return(force, direction)\n",
    "    \n",
    "\n",
    "# Remplace les valeurs de U,V du dataframe WF_X_train ou WF_X_test en argument par la norme du vent et sa direction\n",
    "def force_direction(WF_X):\n",
    "    # Pour chaque éolienne on parcourt les données, on crée deux array direction et force, on parcourt chaque ligne\n",
    "    # on ajoute la bonne valeur dans l'array. Une fois ceci fait, on supprime les colonnes U et V, on ajoute les colonnes \n",
    "    # direction et force.\n",
    "    \n",
    "    WF = [None]\n",
    "    \n",
    "    for i in range(1,7):\n",
    "        \n",
    "        # Séparation des différentes données, U,V et T, et calcul des moyennes sur une unique colonne\n",
    "        df = concatene(i, WF_X)\n",
    "        U1 = df.loc[:,['NWP1_U','NWP2_U','NWP3_U']].mean(axis = 1)\n",
    "        V1 = df.loc[:,['NWP1_V','NWP2_V','NWP3_V']].mean(axis = 1)\n",
    "        T = df.loc[:,['NWP1_T','NWP3_T']].mean(axis = 1)\n",
    "            \n",
    "        U2 = df.loc[:,'NWP4_U']\n",
    "        V2 = df.loc[:,'NWP4_V']\n",
    "        CLCT = df.loc[:,'NWP4_CLCT']\n",
    "        \n",
    "\n",
    "        # Calcul du sens et de la direction du vent\n",
    "        force1 = []\n",
    "        direction1 = []\n",
    "        \n",
    "        force2=[]\n",
    "        direction2=[]\n",
    "        \n",
    "        for j in range(df.shape[0]):\n",
    "            u1 = U1.iloc[j]\n",
    "            v1 = V1.iloc[j]\n",
    "            u2 = U2.iloc[j]\n",
    "            v2 = V2.iloc[j]\n",
    "            \n",
    "            [f1,d1] = calcule_force_direction(u1,v1)\n",
    "            [f2,d2] = calcule_force_direction(u2,v2)\n",
    "            \n",
    "            force1.append(f1)\n",
    "            direction1.append(d1)\n",
    "            force2.append(f2)\n",
    "            direction2.append(d2)\n",
    "        \n",
    "        force1 = np.array(force1)\n",
    "        force2 = np.array(force2)\n",
    "        direction1 = np.array(direction1)\n",
    "        direction2 = np.array(direction2)\n",
    "        \n",
    "        new_dataframe = pd.DataFrame({'NWP123_T':T, 'NWP123_force':force1, 'NWP123_dir':direction1, 'NWP_4_force':force2, 'NWP4_dir':direction2, 'NWP4_CLCT':CLCT }) \n",
    "        \n",
    "        \n",
    "        \n",
    "        WF.append(new_dataframe)\n",
    "    \n",
    "    return(WF)\n",
    "\n",
    "def coordonnees_norme(WF_X):\n",
    "    # Pour chaque éolienne on parcourt les données, on crée deux array direction et force, on parcourt chaque ligne\n",
    "    # on ajoute la bonne valeur dans l'array. Une fois ceci fait, on supprime les colonnes U et V, on ajoute les colonnes \n",
    "    # direction et force.\n",
    "    \n",
    "    WF = [None]\n",
    "    \n",
    "    for i in range(1,7):\n",
    "        \n",
    "        # Séparation des différentes données, U,V et T, et calcul des moyennes sur une unique colonne\n",
    "        df = concatene(i, WF_X)\n",
    "        U1 = df.loc[:,['NWP1_U','NWP2_U','NWP3_U']].mean(axis = 1)\n",
    "        V1 = df.loc[:,['NWP1_V','NWP2_V','NWP3_V']].mean(axis = 1)\n",
    "        T = df.loc[:,['NWP1_T','NWP3_T']].mean(axis = 1)\n",
    "            \n",
    "        U2 = df.loc[:,'NWP4_U']\n",
    "        V2 = df.loc[:,'NWP4_V']\n",
    "        CLCT = df.loc[:,'NWP4_CLCT']\n",
    "        \n",
    "\n",
    "        # Calcul du sens et de la direction du vent\n",
    "        norme1 = []\n",
    "        \n",
    "        norme2 =[]\n",
    "        \n",
    "        for j in range(df.shape[0]):\n",
    "            u1 = U1.iloc[j]\n",
    "            v1 = V1.iloc[j]\n",
    "            u2 = U2.iloc[j]\n",
    "            v2 = V2.iloc[j]\n",
    "            \n",
    "            n1 = np.sqrt(u1**2 + v1**2)\n",
    "            n2 = np.sqrt(u2**2 + v2**2)\n",
    "            \n",
    "            norme1.append(n1)\n",
    "            norme2.append(n2)\n",
    "\n",
    "        \n",
    "        norme1 = np.array(norme1)\n",
    "        norme2 = np.array(norme2)\n",
    "        \n",
    "        new_dataframe = pd.DataFrame({'NWP123_T':T, 'NWP123_U':U1 , 'NWP123_V':V1, 'NWP123_norme':norme1, 'NWP4_U':U2, 'NWP4_V':V2, 'NWP4_norme':norme2, 'NWP4_CLCT':CLCT }) \n",
    "        \n",
    "        WF.append(new_dataframe)\n",
    "    \n",
    "    return(WF)\n",
    "\n",
    "\n",
    "def sans_T(df):\n",
    "    for x in df.columns:\n",
    "        if '_T' in x :\n",
    "            df = df.drop([x], axis=1)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement de l'algorithme, et prédiction des valeurs de y_test, puis enregistrement du fichier pour submition\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "WF_y_pred = []\n",
    "for i in range(1,7):\n",
    "    \n",
    "    # Récupération des données d'entrainement\n",
    "    X_train = concatene(i,WF_X_train)\n",
    "    y_train = np.ravel(WF_Y_train[i])\n",
    "    \n",
    "    # Définition de l'algorithme\n",
    "    regressor = RandomForestRegressor(n_estimators=1000)\n",
    "    \n",
    "    # Entrainement\n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Préparation des données de test et prédiction\n",
    "    X_test = concatene(i,WF_X_test)\n",
    "    y_pred = regressor.predict(X_test)\n",
    "    \n",
    "    WF_y_pred.append(y_pred)\n",
    "\n",
    "    \n",
    "# Formation d'un unique vecteur y_pred\n",
    "y_pred = np.array([])\n",
    "for i in range(6):\n",
    "    y_pred = np.hstack((y_pred,WF_y_pred[i]))\n",
    "\n",
    "# Mise en forme du fichier\n",
    "# Crée un dataframe vide, créer une colonne prenant les valeur de y_pred, réindex selon les valeurs attendues pour submit\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Production'] = y_pred\n",
    "index_start = 37376\n",
    "df = df.rename(index= lambda s: s + index_start)\n",
    "\n",
    "# Enregistre le dataframe dans un csv coma-separated du nom de y_pred.csv dans le repertoire de travail.\n",
    "df.to_csv('y_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1754528449530183"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrainement de l'algorithme, puis prédiction des valeurs de y_train, et calcul de l'erreur (pour test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "WF_y_pred = []\n",
    "\n",
    "# Toujours plus de noms de variables\n",
    "WF_X_train_tot = force_direction(WF_X_train)\n",
    "\n",
    "for i in range(1,7):\n",
    "    y_train = np.ravel(WF_Y_train[i])\n",
    "    \n",
    "    X_train = WF_X_train_tot[i]\n",
    "    \n",
    "    regressor = RandomForestRegressor(n_estimators=1000, random_state=0, oob_score=True)\n",
    "    \n",
    "    regressor.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = regressor.predict(X_train)\n",
    "    \n",
    "    WF_y_pred.append(y_pred)\n",
    "\n",
    "    \n",
    "# Formation d'un unique vecteur y_pred\n",
    "y_pred = np.array([])\n",
    "\n",
    "for i in range(6):\n",
    "    \n",
    "    y_pred = np.hstack((y_pred,WF_y_pred[i]))\n",
    "\n",
    "    \n",
    "# Mise en forme de y_train et calcul de l'erreur de prédiction sur le set de test.\n",
    "\n",
    "y_train = np.ravel(Y_train_tot)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_train, y_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6aa4830e21701a39a418475b3c4e549b48bdbdb10aa340300157378144547db0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
